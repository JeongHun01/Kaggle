{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a248c12e-76a5-4d89-833a-86dc0adc3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/test.csv')\n",
    "sub_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5191cbbe-79d6-4333-a629-02deb0ce2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that won't be used to make model\n",
    "def delete_features(df):\n",
    "    feature_list = ['PassengerId', 'Ticket', 'Cabin']\n",
    "    df.drop(feature_list, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Remove outlier data at Age\n",
    "def remove_age(df):\n",
    "    index = df[df['Age'] >= 80].index\n",
    "    df = df.drop(index, axis = 0, inplace = False)\n",
    "\n",
    "\n",
    "# Fill Null Value at train data\n",
    "def fill_NaN_train(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "\n",
    "    df['Embarked'] = df['Embarked'].fillna('C')\n",
    "\n",
    "# Fill Null Value at train test\n",
    "def fill_NaN_test(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "            \n",
    "    df['Fare'][152] = 28.230436\n",
    "\n",
    "\n",
    "# Extract title from Name, remove titles that not exist in test data and do encoding at train data\n",
    "def Name_Engineering_train(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "\n",
    "    df['Title'] = Title_list\n",
    "    drop_title = ['Mlle', 'Major', 'the Countess', 'Capt', 'Sir', 'Lady', 'Mme', 'Don', 'Jonkheer']\n",
    "    drop_index = list()\n",
    "    for title in drop_title:\n",
    "        index = df[df['Title'] == title].index.tolist()\n",
    "        drop_index.append(index)\n",
    "    drop_index = sum(drop_index,[])\n",
    "    df.drop(drop_index, axis = 0, inplace = True)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_title = label.fit_transform(df['Title'])\n",
    "    df['Title'] = new_title\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "# Extract title from Name and do encoding at test data\n",
    "def Name_Engineering_test(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "        \n",
    "    # Dona index = 414 change Dona to ZDona for last labeling (not in train set)\n",
    "    Title_list[414] = 'ZDona'\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_title = label.fit_transform(Title_list)\n",
    "    df['Title'] = new_title\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Encoding the Sex feature\n",
    "def Sex_Encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_sex = label.fit_transform(df['Sex'])\n",
    "    df['Sex'] = new_sex\n",
    "\n",
    "\n",
    "# Transform continous data to 9 selections and Encoding\n",
    "def Age_Engineering(df):\n",
    "    def Age_Conversion(age):\n",
    "        title = ''\n",
    "        if age <= 5:\n",
    "            title = 'Baby'\n",
    "        elif age <= 16:\n",
    "            title = 'Child'\n",
    "        elif age <= 32:\n",
    "            title = 'Young_Adult'\n",
    "        elif age <= 48:\n",
    "            title = 'Adult'\n",
    "        elif age <= 64:\n",
    "            title = 'Old_Adult'\n",
    "        else:\n",
    "            title = 'Senior'\n",
    "\n",
    "        return title\n",
    "\n",
    "    df['Age_selection'] = df['Age'].apply(lambda x : Age_Conversion(x))\n",
    "    df.drop('Age', axis = 1, inplace = True)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_age = label.fit_transform(df['Age_selection'])\n",
    "    df['Age_selection'] = new_age\n",
    "\n",
    "\n",
    "# Combine SibSp feature and Parch feature to make Family new feature\n",
    "def Family_Conversion(df):\n",
    "    df['Family'] = df['SibSp'] + df['Parch']\n",
    "    df.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Do log conversion on Fare feature to reduce right skewness\n",
    "def Fare_Log(df):\n",
    "    df['Fare'] = np.log1p(df['Fare'])\n",
    "\n",
    "\n",
    "# Encoding Embarked feature\n",
    "def Embarked_Encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_embarked = label.fit_transform(df['Embarked'])\n",
    "    df['Embarked'] = new_embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f76029a-f83f-42f2-83c4-7499d9e01ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_train(df):\n",
    "    delete_features(df)\n",
    "    remove_age(df)\n",
    "    fill_NaN_train(df)\n",
    "    Name_Engineering_train(df)\n",
    "    Sex_Encoding(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)\n",
    "    Embarked_Encoding(df)\n",
    "\n",
    "def Feature_Engineering_test(df):\n",
    "    delete_features(df)\n",
    "    fill_NaN_test(df)\n",
    "    Name_Engineering_test(df)\n",
    "    Sex_Encoding(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)\n",
    "    Embarked_Encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ee8814-d3e3-42e0-993b-6f3b5e1ffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "Feature_Engineering_train(train_df)\n",
    "Feature_Engineering_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9c9812-0117-48b4-923b-0b65249534fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = train_df.drop('Survived', axis = 1, inplace = False)\n",
    "Label = train_df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad62d0-c419-4a09-a5ef-5f8e3e550b13",
   "metadata": {},
   "source": [
    "**사용할 모델들** : Decision Tree, Random Forest, XGBoost, LightGBM, Logistic Regression\n",
    "\n",
    "Hard Voting, Soft Voting 사용 비교 후 더 좋은 성능의 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a4b88c-fdad-41da-b854-adc9d5752451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0)\n",
    "rf_model = RandomForestClassifier(random_state = 0)\n",
    "xgb_model = XGBClassifier(random_state = 0)\n",
    "lgb_model = LGBMClassifier(random_state = 0)\n",
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704b237c-8093-4d30-82bf-ce423afe7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 정확도 :  0.8011363636363636\n",
      "Random Forest 정확도 :  0.8056818181818182\n",
      "XGBoost 정확도 :  0.8238636363636364\n",
      "LightGBM 정확도 :  0.8011363636363636\n",
      "Logistic Regression 정확도 :  0.8011363636363636\n"
     ]
    }
   ],
   "source": [
    "# Cheking None-Tuning models performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt_score = cross_val_score(dt_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Decision Tree 정확도 : \", np.mean(dt_score))\n",
    "\n",
    "rf_score = cross_val_score(rf_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Random Forest 정확도 : \", np.mean(rf_score))\n",
    "\n",
    "xgb_score = cross_val_score(xgb_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"XGBoost 정확도 : \", np.mean(xgb_score))\n",
    "\n",
    "lgb_score = cross_val_score(lgb_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"LightGBM 정확도 : \", np.mean(dt_score))\n",
    "\n",
    "lr_score = cross_val_score(lr_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Logistic Regression 정확도 : \", np.mean(dt_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be99336f-2e44-454d-86d3-5095c3dd0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 정확도 : 0.8\n",
      "Random Forest 정확도 : 0.8147727272727273\n",
      "XGBoost 정확도 : 0.8375\n",
      "LightGBM 정확도 : 0.8284090909090909\n",
      "Logistic Regression 정확도 : 0.7874999999999999\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - removed Embarked\n",
    "drop_Feature = Feature.drop(['Embarked'], axis = 1, inplace = False)\n",
    "drop_test = train_df.drop(['Embarked'], axis = 1, inplace = False)\n",
    "\n",
    "def accuracy_model(model_list, name_list):\n",
    "    for i, model in enumerate(model_list):\n",
    "        score = cross_val_score(model, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "        print(\"{0} 정확도 : {1}\".format(name_list[i], np.mean(score)))\n",
    "\n",
    "model_list = [dt_model, rf_model, xgb_model, lgb_model, lr_model]\n",
    "name_list = ['Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Logistic Regression']\n",
    "\n",
    "accuracy_model(model_list, name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a14ca756-e446-4d19-9f1d-cd8d83bf7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter tuning\n",
    "\n",
    "random_seed = [0]\n",
    "\n",
    "# DecisionTree model hyper parameter\n",
    "dt_params = {'max_depth' : [ i for i in range(2,11) ],\n",
    "             'criterion' : ['gini', 'entropy'],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# RandomForest model hyper parameter\n",
    "rf_params = {'n_estimators' : [50, 100, 150, 200],\n",
    "             'max_depth' : [i for i in range(2,11)],\n",
    "             'criterion' : ['gini', 'entropy'],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# XGBoost model hyper parameter\n",
    "xgb_params = {'max_depth' :[i for i in range(2,11)],\n",
    "              'min_child_weight' : [2 * i for i in range(2,16)],\n",
    "              'random_state' : random_seed}\n",
    "\n",
    "# LightGBM model hyper parameter\n",
    "lgb_params = {'max_depth' : [i for i in range(2,11)],\n",
    "             'num_leaves' : [2 * i for i in range(2,21)],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# Logistic Regression model hyper parameter\n",
    "lr_params = {'penalty' : ['l1', 'l2'],\n",
    "             'C' : [0.01, 0.1, 0.05, 1, 5, 10],\n",
    "             'solver' : ['lbfgs', 'liblinear']}\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222fe585-5df1-46fe-9d6c-e4edb529c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 최적 파라미터 : {'criterion': 'gini', 'max_depth': 5, 'random_state': 0}, 이때 정확도 : 0.8329545454545455\n",
      "Random Forest 최적 파라미터 : {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 50, 'random_state': 0}, 이때 정확도 : 0.8306818181818182\n",
      "XGBoost 최적 파라미터 : {'max_depth': 4, 'min_child_weight': 6, 'random_state': 0}, 이때 정확도 : 0.8511363636363637\n",
      "LightGBM 최적 파라미터 : {'max_depth': 7, 'num_leaves': 14, 'random_state': 0}, 이때 정확도 : 0.8443181818181816\n",
      "Logistic Regression 최적 파라미터 : {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}, 이때 정확도 : 0.7897727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_dt = GridSearchCV(dt_model, param_grid = dt_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_dt.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Decision Tree', tuned_dt.best_params_, tuned_dt.best_score_))\n",
    "\n",
    "tuned_rf = GridSearchCV(rf_model, param_grid = rf_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_rf.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Random Forest', tuned_rf.best_params_, tuned_rf.best_score_))\n",
    "\n",
    "tuned_xgb = GridSearchCV(xgb_model, param_grid = xgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_xgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('XGBoost', tuned_xgb.best_params_, tuned_xgb.best_score_))\n",
    "\n",
    "tuned_lgb = GridSearchCV(lgb_model, param_grid = lgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('LightGBM', tuned_lgb.best_params_, tuned_lgb.best_score_))\n",
    "\n",
    "tuned_lr = GridSearchCV(lr_model, param_grid = lr_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lr.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Logistic Regression', tuned_lr.best_params_, tuned_lr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf09722-06fe-4b23-83f3-f5edc38a3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 최적 파라미터 : {'C': 2, 'penalty': 'l2', 'solver': 'lbfgs'}, 이때 정확도 : 0.7886363636363636\n"
     ]
    }
   ],
   "source": [
    "lr_params = {'penalty' : ['l1', 'l2'],\n",
    "             'C' : [1,2,3,4,5,6,7,8,9,10],\n",
    "             'solver' : ['lbfgs', 'liblinear']}\n",
    "\n",
    "tuned_lr = GridSearchCV(lr_model, param_grid = lr_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lr.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Logistic Regression', tuned_lr.best_params_, tuned_lr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a919e4-f6b8-4799-bd64-f99cdfeb8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning learning_rate\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials\n",
    "\n",
    "xgb_lr = {'learning_rate' : hp.uniform('learning_rate', 0.01,0.2)}\n",
    "lgb_lr = {'learning_rate' : hp.uniform('learning_rate', 0.01,0.2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093dffcc-5261-415a-80cc-4c4dbaf7769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def xgb_func(space):\n",
    "    xgb_model = XGBClassifier(max_depth = 4, min_child_weight = 6, random_state = 0, learning_rate = space['learning_rate'])\n",
    "\n",
    "    score = cross_val_score(xgb_model, drop_Feature, Label, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "    return {'loss' : -1 * np.mean(score), 'status' : STATUS_OK}\n",
    "\n",
    "def lgb_func(space):\n",
    "    lgb_model = LGBMClassifier(max_depth = 7, num_leaves = 14, random_state = 0, learning_rate = space['learning_rate'])\n",
    "\n",
    "    score = cross_val_score(lgb_model, drop_Feature, Label, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "    return {'loss' : -1 * np.mean(score), 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125a6c31-76d9-47ea-bca3-15b03669e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 200/200 [02:20<00:00,  1.42trial/s, best loss: -0.8511363636363637]\n",
      "100%|█████████████████████████████████████████████| 200/200 [03:02<00:00,  1.10trial/s, best loss: -0.8488636363636364]\n"
     ]
    }
   ],
   "source": [
    "trials1 = Trials()\n",
    "trials2 = Trials()\n",
    "\n",
    "best_xgb = fmin(fn = xgb_func, space = xgb_lr, algo = tpe.suggest, max_evals = 200, trials = trials1)\n",
    "best_lgb = fmin(fn = lgb_func, space = lgb_lr, algo = tpe.suggest, max_evals = 200, trials = trials2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29dc604b-16ac-4675-a976-660a0865aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 XGBoost learning_rate :  {'learning_rate': 0.19099427768421384}\n",
      "최적 LightGBM learning_rate :  {'learning_rate': 0.1395954806287981}\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 XGBoost learning_rate : \", best_xgb)\n",
    "print(\"최적 LightGBM learning_rate : \", best_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49e0c43d-121a-42d3-9b89-3a086c5b4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting 정확도 :  0.8477272727272727\n",
      "Soft Voting 정확도 :  0.834090909090909\n"
     ]
    }
   ],
   "source": [
    "# voting\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0, max_depth = 5, criterion = 'gini')\n",
    "rf_model = RandomForestClassifier(random_state = 0, criterion = 'entropy', max_depth = 5, n_estimators = 50)\n",
    "xgb_model = XGBClassifier(random_state = 0, max_depth = 4, min_child_weight = 6, learning_rate = 0.19099427768421384)\n",
    "lgb_model = LGBMClassifier(random_state = 0, max_depth = 7, num_leaves = 14, learning_rate = 0.1395954806287981)\n",
    "#lr_model = LogisticRegression(C = 2, penalty = 'l2', solver = 'lbfgs')\n",
    "\n",
    "# Hard Voting\n",
    "hard_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], \n",
    "                             voting = 'hard')\n",
    "hard_score = cross_val_score(hard_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Hard Voting 정확도 : \", np.mean(hard_score))\n",
    "\n",
    "# Soft Voting\n",
    "soft_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], \n",
    "                             voting = 'soft')\n",
    "soft_score = cross_val_score(soft_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Soft Voting 정확도 : \", np.mean(soft_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7753a06-af1d-49c4-8408-b3f3edea49b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
