{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d2d86a-774b-4e42-a2f9-22b932411d64",
   "metadata": {},
   "source": [
    "## **Third Attempt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a248c12e-76a5-4d89-833a-86dc0adc3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/test.csv')\n",
    "sub_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5191cbbe-79d6-4333-a629-02deb0ce2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that won't be used to make model\n",
    "def delete_features(df):\n",
    "    feature_list = ['PassengerId', 'Ticket', 'Cabin']\n",
    "    df.drop(feature_list, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Remove outlier data at Age\n",
    "def remove_age(df):\n",
    "    index = df[df['Age'] >= 80].index\n",
    "    df = df.drop(index, axis = 0, inplace = False)\n",
    "\n",
    "\n",
    "# Fill Null Value at train data\n",
    "def fill_NaN_train(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "\n",
    "    df['Embarked'] = df['Embarked'].fillna('C')\n",
    "\n",
    "# Fill Null Value at train test\n",
    "def fill_NaN_test(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "            \n",
    "    df['Fare'][152] = 28.230436\n",
    "\n",
    "\n",
    "# Extract title from Name, remove titles that not exist in test data and do encoding at train data\n",
    "def Name_Engineering_train(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "\n",
    "    df['Title'] = Title_list\n",
    "    drop_title = ['Mlle', 'Major', 'the Countess', 'Capt', 'Sir', 'Lady', 'Mme', 'Don', 'Jonkheer']\n",
    "    drop_index = list()\n",
    "    for title in drop_title:\n",
    "        index = df[df['Title'] == title].index.tolist()\n",
    "        drop_index.append(index)\n",
    "    drop_index = sum(drop_index,[])\n",
    "    df.drop(drop_index, axis = 0, inplace = True)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_title = label.fit_transform(df['Title'])\n",
    "    df['Title'] = new_title\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "# Extract title from Name and do encoding at test data\n",
    "def Name_Engineering_test(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "        \n",
    "    # Dona index = 414 change Dona to ZDona for last labeling (not in train set)\n",
    "    Title_list[414] = 'ZDona'\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_title = label.fit_transform(Title_list)\n",
    "    df['Title'] = new_title\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Encoding the Sex feature\n",
    "def Sex_Encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_sex = label.fit_transform(df['Sex'])\n",
    "    df['Sex'] = new_sex\n",
    "\n",
    "\n",
    "# Transform continous data to 9 selections and Encoding\n",
    "def Age_Engineering(df):\n",
    "    def Age_Conversion(age):\n",
    "        title = ''\n",
    "        if age <= 5:\n",
    "            title = 'Baby'\n",
    "        elif age <= 16:\n",
    "            title = 'Child'\n",
    "        elif age <= 32:\n",
    "            title = 'Young_Adult'\n",
    "        elif age <= 48:\n",
    "            title = 'Adult'\n",
    "        elif age <= 64:\n",
    "            title = 'Old_Adult'\n",
    "        else:\n",
    "            title = 'Senior'\n",
    "\n",
    "        return title\n",
    "\n",
    "    df['Age_selection'] = df['Age'].apply(lambda x : Age_Conversion(x))\n",
    "    df.drop('Age', axis = 1, inplace = True)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_age = label.fit_transform(df['Age_selection'])\n",
    "    df['Age_selection'] = new_age\n",
    "\n",
    "\n",
    "# Combine SibSp feature and Parch feature to make Family new feature\n",
    "def Family_Conversion(df):\n",
    "    df['Family'] = df['SibSp'] + df['Parch']\n",
    "    df.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Do log conversion on Fare feature to reduce right skewness\n",
    "def Fare_Log(df):\n",
    "    df['Fare'] = np.log1p(df['Fare'])\n",
    "\n",
    "\n",
    "# Encoding Embarked feature\n",
    "def Embarked_Encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_embarked = label.fit_transform(df['Embarked'])\n",
    "    df['Embarked'] = new_embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f76029a-f83f-42f2-83c4-7499d9e01ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_train(df):\n",
    "    delete_features(df)\n",
    "    remove_age(df)\n",
    "    fill_NaN_train(df)\n",
    "    Name_Engineering_train(df)\n",
    "    Sex_Encoding(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)\n",
    "    Embarked_Encoding(df)\n",
    "\n",
    "def Feature_Engineering_test(df):\n",
    "    delete_features(df)\n",
    "    fill_NaN_test(df)\n",
    "    Name_Engineering_test(df)\n",
    "    Sex_Encoding(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)\n",
    "    Embarked_Encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ee8814-d3e3-42e0-993b-6f3b5e1ffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "Feature_Engineering_train(train_df)\n",
    "Feature_Engineering_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9c9812-0117-48b4-923b-0b65249534fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = train_df.drop('Survived', axis = 1, inplace = False)\n",
    "Label = train_df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad62d0-c419-4a09-a5ef-5f8e3e550b13",
   "metadata": {},
   "source": [
    "**사용할 모델들** : Decision Tree, Random Forest, XGBoost, LightGBM, Logistic Regression\n",
    "\n",
    "Hard Voting, Soft Voting 사용 비교 후 더 좋은 성능의 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a4b88c-fdad-41da-b854-adc9d5752451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0)\n",
    "rf_model = RandomForestClassifier(random_state = 0)\n",
    "xgb_model = XGBClassifier(random_state = 0)\n",
    "lgb_model = LGBMClassifier(random_state = 0)\n",
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704b237c-8093-4d30-82bf-ce423afe7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 정확도 :  0.8011363636363636\n",
      "Random Forest 정확도 :  0.8056818181818182\n",
      "XGBoost 정확도 :  0.8238636363636364\n",
      "LightGBM 정확도 :  0.8011363636363636\n",
      "Logistic Regression 정확도 :  0.8011363636363636\n"
     ]
    }
   ],
   "source": [
    "# Cheking None-Tuning models performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt_score = cross_val_score(dt_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Decision Tree 정확도 : \", np.mean(dt_score))\n",
    "\n",
    "rf_score = cross_val_score(rf_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Random Forest 정확도 : \", np.mean(rf_score))\n",
    "\n",
    "xgb_score = cross_val_score(xgb_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"XGBoost 정확도 : \", np.mean(xgb_score))\n",
    "\n",
    "lgb_score = cross_val_score(lgb_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"LightGBM 정확도 : \", np.mean(dt_score))\n",
    "\n",
    "lr_score = cross_val_score(lr_model, Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Logistic Regression 정확도 : \", np.mean(dt_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be99336f-2e44-454d-86d3-5095c3dd0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 정확도 : 0.8329545454545455\n",
      "Random Forest 정확도 : 0.8306818181818182\n",
      "XGBoost 정확도 : 0.8511363636363637\n",
      "LightGBM 정확도 : 0.8488636363636364\n",
      "Logistic Regression 정확도 : 0.7874999999999999\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - removed Embarked\n",
    "drop_Feature = Feature.drop(['Embarked'], axis = 1, inplace = False)\n",
    "drop_test = test_df.drop(['Embarked'], axis = 1, inplace = False)\n",
    "\n",
    "def accuracy_model(model_list, name_list):\n",
    "    for i, model in enumerate(model_list):\n",
    "        score = cross_val_score(model, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "        print(\"{0} 정확도 : {1}\".format(name_list[i], np.mean(score)))\n",
    "\n",
    "model_list = [dt_model, rf_model, xgb_model, lgb_model, lr_model]\n",
    "name_list = ['Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Logistic Regression']\n",
    "\n",
    "accuracy_model(model_list, name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a14ca756-e446-4d19-9f1d-cd8d83bf7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter tuning\n",
    "\n",
    "random_seed = [0]\n",
    "\n",
    "# DecisionTree model hyper parameter\n",
    "dt_params = {'max_depth' : [ i for i in range(2,11) ],\n",
    "             'criterion' : ['gini', 'entropy'],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# RandomForest model hyper parameter\n",
    "rf_params = {'n_estimators' : [50, 100, 150, 200],\n",
    "             'max_depth' : [i for i in range(2,11)],\n",
    "             'criterion' : ['gini', 'entropy'],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# XGBoost model hyper parameter\n",
    "xgb_params = {'max_depth' :[i for i in range(2,11)],\n",
    "              'min_child_weight' : [2 * i for i in range(2,16)],\n",
    "              'random_state' : random_seed}\n",
    "\n",
    "# LightGBM model hyper parameter\n",
    "lgb_params = {'max_depth' : [i for i in range(2,11)],\n",
    "             'num_leaves' : [2 * i for i in range(2,21)],\n",
    "             'random_state' : random_seed}\n",
    "\n",
    "# Logistic Regression model hyper parameter\n",
    "lr_params = {'penalty' : ['l1', 'l2'],\n",
    "             'C' : [0.01, 0.1, 0.05, 1, 5, 10],\n",
    "             'solver' : ['lbfgs', 'liblinear']}\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222fe585-5df1-46fe-9d6c-e4edb529c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 최적 파라미터 : {'criterion': 'gini', 'max_depth': 5, 'random_state': 0}, 이때 정확도 : 0.8329545454545455\n",
      "Random Forest 최적 파라미터 : {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 50, 'random_state': 0}, 이때 정확도 : 0.8306818181818182\n",
      "XGBoost 최적 파라미터 : {'max_depth': 4, 'min_child_weight': 6, 'random_state': 0}, 이때 정확도 : 0.8511363636363637\n",
      "LightGBM 최적 파라미터 : {'max_depth': 7, 'num_leaves': 14, 'random_state': 0}, 이때 정확도 : 0.8443181818181816\n",
      "Logistic Regression 최적 파라미터 : {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}, 이때 정확도 : 0.7897727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_dt = GridSearchCV(dt_model, param_grid = dt_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_dt.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Decision Tree', tuned_dt.best_params_, tuned_dt.best_score_))\n",
    "\n",
    "tuned_rf = GridSearchCV(rf_model, param_grid = rf_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_rf.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Random Forest', tuned_rf.best_params_, tuned_rf.best_score_))\n",
    "\n",
    "tuned_xgb = GridSearchCV(xgb_model, param_grid = xgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_xgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('XGBoost', tuned_xgb.best_params_, tuned_xgb.best_score_))\n",
    "\n",
    "tuned_lgb = GridSearchCV(lgb_model, param_grid = lgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('LightGBM', tuned_lgb.best_params_, tuned_lgb.best_score_))\n",
    "\n",
    "tuned_lr = GridSearchCV(lr_model, param_grid = lr_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lr.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Logistic Regression', tuned_lr.best_params_, tuned_lr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf09722-06fe-4b23-83f3-f5edc38a3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 최적 파라미터 : {'C': 2, 'penalty': 'l2', 'solver': 'lbfgs'}, 이때 정확도 : 0.7886363636363636\n"
     ]
    }
   ],
   "source": [
    "lr_params = {'penalty' : ['l1', 'l2'],\n",
    "             'C' : [1,2,3,4,5,6,7,8,9,10],\n",
    "             'solver' : ['lbfgs', 'liblinear']}\n",
    "\n",
    "tuned_lr = GridSearchCV(lr_model, param_grid = lr_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lr.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Logistic Regression', tuned_lr.best_params_, tuned_lr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a919e4-f6b8-4799-bd64-f99cdfeb8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning learning_rate\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials\n",
    "\n",
    "xgb_lr = {'learning_rate' : hp.uniform('learning_rate', 0.01,0.2)}\n",
    "lgb_lr = {'learning_rate' : hp.uniform('learning_rate', 0.01,0.2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093dffcc-5261-415a-80cc-4c4dbaf7769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def xgb_func(space):\n",
    "    xgb_model = XGBClassifier(max_depth = 4, min_child_weight = 6, random_state = 0, learning_rate = space['learning_rate'])\n",
    "\n",
    "    score = cross_val_score(xgb_model, drop_Feature, Label, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "    return {'loss' : -1 * np.mean(score), 'status' : STATUS_OK}\n",
    "\n",
    "def lgb_func(space):\n",
    "    lgb_model = LGBMClassifier(max_depth = 7, num_leaves = 14, random_state = 0, learning_rate = space['learning_rate'])\n",
    "\n",
    "    score = cross_val_score(lgb_model, drop_Feature, Label, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "    return {'loss' : -1 * np.mean(score), 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125a6c31-76d9-47ea-bca3-15b03669e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 200/200 [02:20<00:00,  1.42trial/s, best loss: -0.8511363636363637]\n",
      "100%|█████████████████████████████████████████████| 200/200 [03:02<00:00,  1.10trial/s, best loss: -0.8488636363636364]\n"
     ]
    }
   ],
   "source": [
    "trials1 = Trials()\n",
    "trials2 = Trials()\n",
    "\n",
    "best_xgb = fmin(fn = xgb_func, space = xgb_lr, algo = tpe.suggest, max_evals = 200, trials = trials1)\n",
    "best_lgb = fmin(fn = lgb_func, space = lgb_lr, algo = tpe.suggest, max_evals = 200, trials = trials2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29dc604b-16ac-4675-a976-660a0865aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 XGBoost learning_rate :  {'learning_rate': 0.19099427768421384}\n",
      "최적 LightGBM learning_rate :  {'learning_rate': 0.1395954806287981}\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 XGBoost learning_rate : \", best_xgb)\n",
    "print(\"최적 LightGBM learning_rate : \", best_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49e0c43d-121a-42d3-9b89-3a086c5b4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting 정확도 :  0.8477272727272727\n",
      "Soft Voting 정확도 :  0.834090909090909\n"
     ]
    }
   ],
   "source": [
    "# voting\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0, max_depth = 5, criterion = 'gini')\n",
    "rf_model = RandomForestClassifier(random_state = 0, criterion = 'entropy', max_depth = 5, n_estimators = 50)\n",
    "xgb_model = XGBClassifier(random_state = 0, max_depth = 4, min_child_weight = 6, learning_rate = 0.19099427768421384)\n",
    "lgb_model = LGBMClassifier(random_state = 0, max_depth = 7, num_leaves = 14, learning_rate = 0.1395954806287981)\n",
    "#lr_model = LogisticRegression(C = 2, penalty = 'l2', solver = 'lbfgs')\n",
    "\n",
    "# Hard Voting\n",
    "hard_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], \n",
    "                             voting = 'hard')\n",
    "hard_score = cross_val_score(hard_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Hard Voting 정확도 : \", np.mean(hard_score))\n",
    "\n",
    "# Soft Voting\n",
    "soft_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], \n",
    "                             voting = 'soft')\n",
    "soft_score = cross_val_score(soft_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Soft Voting 정확도 : \", np.mean(soft_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7753a06-af1d-49c4-8408-b3f3edea49b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hard_vote.fit(drop_Feature, Label)\n",
    "prediction = hard_vote.predict(drop_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "\"PassengerId\":sub_df['PassengerId'],\n",
    "\"Survived\": prediction\n",
    "})\n",
    "display(submission)\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6e215-3c4f-4d0c-84a3-36c4e411be7a",
   "metadata": {},
   "source": [
    "**score** : 0.77990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39324231-4d38-40f7-adae-596b819020f3",
   "metadata": {},
   "source": [
    "**OneHot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "064bf62a-4cf3-42e1-aeac-1b1fcd87cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/test.csv')\n",
    "sub_df = pd.read_csv('C:/Users/user/Desktop/Study/Kaggle/Titanic/Data_File/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df4052ca-7a50-4979-99c1-744e98cfbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that won't be used to make model\n",
    "def delete_features(df):\n",
    "    feature_list = ['PassengerId', 'Ticket', 'Cabin']\n",
    "    df.drop(feature_list, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Remove outlier data at Age\n",
    "def remove_age(df):\n",
    "    index = df[df['Age'] >= 80].index\n",
    "    df = df.drop(index, axis = 0, inplace = False)\n",
    "\n",
    "\n",
    "# Fill Null Value at train data\n",
    "def fill_NaN_train(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "\n",
    "    df['Embarked'] = df['Embarked'].fillna('C')\n",
    "\n",
    "# Fill Null Value at train test\n",
    "def fill_NaN_test(df):\n",
    "    index_list = df[df['Age'].isna() == 1].index.tolist()\n",
    "    for index in index_list:\n",
    "        Pclass = df[df.index == index]['Pclass'].values.tolist()[0]\n",
    "        if Pclass == 1:\n",
    "            df['Age'][index] = df[df['Pclass'] == 1].loc[:,'Age'].mean()\n",
    "        elif Pclass == 2:\n",
    "            df['Age'][index] = df[df['Pclass'] == 2].loc[:,'Age'].mean()\n",
    "        else:\n",
    "            df['Age'][index] = df[df['Pclass'] == 3].loc[:,'Age'].mean()\n",
    "            \n",
    "    df['Fare'][152] = 28.230436\n",
    "\n",
    "\n",
    "# Extract title from Name, remove titles that not exist in test data\n",
    "def Name_Engineering_train(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "\n",
    "    df['Title'] = Title_list\n",
    "    drop_title = ['Mlle', 'Major', 'the Countess', 'Capt', 'Sir', 'Lady', 'Mme', 'Don', 'Jonkheer']\n",
    "    drop_index = list()\n",
    "    for title in drop_title:\n",
    "        index = df[df['Title'] == title].index.tolist()\n",
    "        drop_index.append(index)\n",
    "    drop_index = sum(drop_index,[])\n",
    "    df.drop(drop_index, axis = 0, inplace = True)\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "# Extract title from Name\n",
    "def Name_Engineering_test(df):\n",
    "    Title_list = list()\n",
    "    for str in df['Name']:\n",
    "        str1 = str.split(', ')[1]\n",
    "        str2 = str1.split('.')[0]\n",
    "        Title_list.append(str2)\n",
    "    df['Title'] = Title_list\n",
    "    df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "# Encoding the Sex feature\n",
    "def Sex_Encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label = LabelEncoder()\n",
    "    new_sex = label.fit_transform(df['Sex'])\n",
    "    df['Sex'] = new_sex\n",
    "\n",
    "\n",
    "# Transform continous data to 9 selections and Encoding\n",
    "def Age_Engineering(df):\n",
    "    def Age_Conversion(age):\n",
    "        title = ''\n",
    "        if age <= 5:\n",
    "            title = 'Baby'\n",
    "        elif age <= 16:\n",
    "            title = 'Child'\n",
    "        elif age <= 32:\n",
    "            title = 'Young_Adult'\n",
    "        elif age <= 48:\n",
    "            title = 'Adult'\n",
    "        elif age <= 64:\n",
    "            title = 'Old_Adult'\n",
    "        else:\n",
    "            title = 'Senior'\n",
    "        \n",
    "\n",
    "        return title\n",
    "\n",
    "    df['Age_selection'] = df['Age'].apply(lambda x : Age_Conversion(x))\n",
    "    df.drop('Age', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Combine SibSp feature and Parch feature to make Family new feature\n",
    "def Family_Conversion(df):\n",
    "    df['Family'] = df['SibSp'] + df['Parch']\n",
    "    df.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Do log conversion on Fare feature to reduce right skewness\n",
    "def Fare_Log(df):\n",
    "    df['Fare'] = np.log1p(df['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4df7e775-a1c7-4102-b4d1-56c3d91fed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_train(df):\n",
    "    delete_features(df)\n",
    "    remove_age(df)\n",
    "    fill_NaN_train(df)\n",
    "    Name_Engineering_train(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)\n",
    "\n",
    "def Feature_Engineering_test(df):\n",
    "    delete_features(df)\n",
    "    fill_NaN_test(df)\n",
    "    Name_Engineering_test(df)\n",
    "    Age_Engineering(df)\n",
    "    Family_Conversion(df)\n",
    "    Fare_Log(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f050a0b7-aecc-4f17-abc8-51a37a76b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "Feature_Engineering_train(train_df)\n",
    "Feature_Engineering_test(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5093f469-6fdc-4495-b1c2-e668acfc54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns = ['Title', 'Age_selection', 'Sex'])\n",
    "test_df = pd.get_dummies(test_df, columns = ['Title', 'Age_selection', 'Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e76bff0-c15a-476a-b5ee-29fe8f0da159",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = train_df.drop('Survived', axis = 1, inplace = False)\n",
    "Label = train_df['Survived']\n",
    "\n",
    "drop_Feature = Feature.drop(['Embarked'], axis = 1, inplace = False)\n",
    "drop_test = test_df.drop(['Embarked'], axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b98e661-bd9a-40b2-b40e-dae78b6257a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_test.drop('Title_Dona', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e6820b7-64c5-4dd5-8d65-5defad600e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0)\n",
    "rf_model = RandomForestClassifier(random_state = 0)\n",
    "xgb_model = XGBClassifier(random_state = 0)\n",
    "lgb_model = LGBMClassifier(random_state = 0)\n",
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e188d5c-7c0d-4ad2-824d-d8875eacef48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 정확도 : 0.8034090909090909\n",
      "Random Forest 정확도 : 0.8159090909090908\n",
      "XGBoost 정확도 : 0.8284090909090909\n",
      "LightGBM 정확도 : 0.8420454545454545\n",
      "Logistic Regression 정확도 : 0.8193181818181818\n"
     ]
    }
   ],
   "source": [
    "def accuracy_model(model_list, name_list):\n",
    "    for i, model in enumerate(model_list):\n",
    "        score = cross_val_score(model, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "        print(\"{0} 정확도 : {1}\".format(name_list[i], np.mean(score)))\n",
    "\n",
    "model_list = [dt_model, rf_model, xgb_model, lgb_model, lr_model]\n",
    "name_list = ['Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Logistic Regression']\n",
    "\n",
    "accuracy_model(model_list, name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5406e9e2-2ccc-46d0-8333-a696aaeb4032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 최적 파라미터 : {'criterion': 'entropy', 'max_depth': 4, 'random_state': 0}, 이때 정확도 : 0.8318181818181818\n",
      "Random Forest 최적 파라미터 : {'criterion': 'entropy', 'max_depth': 4, 'n_estimators': 150, 'random_state': 0}, 이때 정확도 : 0.8329545454545455\n",
      "XGBoost 최적 파라미터 : {'max_depth': 6, 'min_child_weight': 8, 'random_state': 0}, 이때 정확도 : 0.8477272727272727\n",
      "LightGBM 최적 파라미터 : {'max_depth': 7, 'num_leaves': 28, 'random_state': 0}, 이때 정확도 : 0.8477272727272727\n",
      "Logistic Regression 최적 파라미터 : {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}, 이때 정확도 : 0.821590909090909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_dt = GridSearchCV(dt_model, param_grid = dt_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_dt.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Decision Tree', tuned_dt.best_params_, tuned_dt.best_score_))\n",
    "\n",
    "tuned_rf = GridSearchCV(rf_model, param_grid = rf_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_rf.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Random Forest', tuned_rf.best_params_, tuned_rf.best_score_))\n",
    "\n",
    "tuned_xgb = GridSearchCV(xgb_model, param_grid = xgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_xgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('XGBoost', tuned_xgb.best_params_, tuned_xgb.best_score_))\n",
    "\n",
    "tuned_lgb = GridSearchCV(lgb_model, param_grid = lgb_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lgb.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('LightGBM', tuned_lgb.best_params_, tuned_lgb.best_score_))\n",
    "\n",
    "tuned_lr = GridSearchCV(lr_model, param_grid = lr_params, cv = 5, scoring = 'accuracy', refit = True)\n",
    "tuned_lr.fit(drop_Feature, Label)\n",
    "print(\"{0} 최적 파라미터 : {1}, 이때 정확도 : {2}\".format('Logistic Regression', tuned_lr.best_params_, tuned_lr.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "015333d1-87fe-4033-9066-12b31ba4891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting 정확도 :  0.8386363636363636\n",
      "Soft Voting 정확도 :  0.8340909090909092\n"
     ]
    }
   ],
   "source": [
    "# voting\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state = 0, max_depth = 4, criterion = 'entropy')\n",
    "rf_model = RandomForestClassifier(random_state = 0, criterion = 'entropy', max_depth = 4, n_estimators = 150)\n",
    "xgb_model = XGBClassifier(random_state = 0, max_depth = 6, min_child_weight = 8)\n",
    "lgb_model = LGBMClassifier(random_state = 0, max_depth = 7, num_leaves = 28)\n",
    "lr_model = LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear')\n",
    "\n",
    "# Hard Voting\n",
    "hard_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model), ('LR', lr_model)], \n",
    "                             voting = 'hard')\n",
    "hard_score = cross_val_score(hard_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Hard Voting 정확도 : \", np.mean(hard_score))\n",
    "\n",
    "# Soft Voting\n",
    "soft_vote = VotingClassifier(estimators = [('DT', dt_model), ('RF', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model), ('LR', lr_model)], \n",
    "                             voting = 'soft')\n",
    "soft_score = cross_val_score(soft_vote, drop_Feature, Label, scoring = 'accuracy', cv = 5)\n",
    "print(\"Soft Voting 정확도 : \", np.mean(soft_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab87ec25-141b-42d4-af69-22d3b8a93f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hard_vote.fit(drop_Feature, Label)\n",
    "prediction = hard_vote.predict(drop_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "\"PassengerId\":sub_df['PassengerId'],\n",
    "\"Survived\": prediction\n",
    "})\n",
    "display(submission)\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fa217-c0a3-4bcb-b827-8a6b8ece0f1f",
   "metadata": {},
   "source": [
    "**score** : 0.78229"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeaf0b-c8bf-41ea-9ac2-9db79c7c3549",
   "metadata": {},
   "source": [
    "-> Data 분석과 Feature Engineering 다시 해보자\n",
    "\n",
    "현재 나의 검증 정확도와 test 정확도와 큰 차이가 나는데 이는 과적합이 되었다고 판단 가능\n",
    "\n",
    "따라서 tuning을 자세히 하되, 먼저 data도 과적합을 벗어나게 단순하게 분류를 해보도록 하자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
